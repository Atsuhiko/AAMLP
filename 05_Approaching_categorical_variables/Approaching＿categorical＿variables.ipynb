{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { \n",
    "    \"Freezing\": 0, \n",
    "    \"Warm\": 1, \n",
    "    \"Cold\": 2, \n",
    "    \"Boiling Hot\": 3, \n",
    "    \"Hot\": 4, \n",
    "    \"Lava Hot\": 5   \n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/train.csv does not exist: '../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-3bb68ccec678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#https://www.kaggle.com/c/cat-in-the-dat-ii/data?select=sample_submission.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ord_2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mord_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/train.csv does not exist: '../input/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#https://www.kaggle.com/c/cat-in-the-dat-ii/data?select=sample_submission.csv\n",
    "df = pd.read_csv(\"../input/train.csv\") \n",
    " \n",
    "df.loc[:, \"ord_2\"] = df.ord_2.map(mapping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    142726\n",
       "1.0    124239\n",
       "2.0     97822\n",
       "3.0     84790\n",
       "4.0     67508\n",
       "5.0     64840\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn import preprocessing \n",
    " \n",
    "# read the data \n",
    "df = pd.read_csv(\"train.csv\") \n",
    " \n",
    "# fill NaN values in ord_2 column \n",
    "df.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\") \n",
    " \n",
    "# initialize LabelEncoder \n",
    "lbl_enc = preprocessing.LabelEncoder() \n",
    " \n",
    "# fit label encoder and transform values on ord_2 column \n",
    "# P.S: do not use this directly. fit first, then transform \n",
    "df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    " \n",
    "# create our example feature matrix \n",
    "example = np.array( \n",
    "    [ \n",
    "        [0, 0, 1], \n",
    "        [1, 0, 0], \n",
    "        [1, 0, 1] \n",
    "    ] \n",
    ") \n",
    " \n",
    "# print size in bytes \n",
    "print(example.nbytes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy import sparse \n",
    " \n",
    "# create our example feature matrix \n",
    "example = np.array( \n",
    "    [ \n",
    "        [0, 0, 1], \n",
    "        [1, 0, 0], \n",
    "        [1, 0, 1] \n",
    "    ] \n",
    ") \n",
    " \n",
    "# convert numpy array to sparse CSR matrix \n",
    "sparse_example = sparse.csr_matrix(example) \n",
    " \n",
    "# print size of this sparse matrix \n",
    "print(sparse_example.data.nbytes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print( \n",
    "    sparse_example.data.nbytes +  \n",
    "    sparse_example.indptr.nbytes +  \n",
    "    sparse_example.indices.nbytes \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 400083040\n",
      "Full size of sparse array: 600164564\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy import sparse \n",
    "\n",
    "# number of rows \n",
    "n_rows = 10000 \n",
    " \n",
    "# number of columns \n",
    "n_cols = 100000 \n",
    " \n",
    "# create random binary matrix with only 5% values as 1s \n",
    "example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols)) \n",
    " \n",
    "# print size in bytes \n",
    "print(f\"Size of dense array: {example.nbytes}\") \n",
    " \n",
    "# convert numpy array to sparse CSR matrix \n",
    "sparse_example = sparse.csr_matrix(example) \n",
    " \n",
    "# print size of this sparse matrix \n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\") \n",
    " \n",
    "full_size = ( \n",
    "    sparse_example.data.nbytes +  \n",
    "    sparse_example.indptr.nbytes +  \n",
    "    sparse_example.indices.nbytes \n",
    ") \n",
    " \n",
    "# print full size of this sparse matrix \n",
    "print(f\"Full size of sparse array: {full_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 144\n",
      "Size of sparse array: 24\n",
      "Full size of sparse array: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy import sparse \n",
    "# create binary matrix \n",
    "example = np.array( \n",
    "    [ \n",
    "        [0, 0, 0, 0, 1, 0], \n",
    "        [0, 1, 0, 0, 0, 0], \n",
    "        [1, 0, 0, 0, 0, 0] \n",
    "    ] \n",
    ") \n",
    " \n",
    "# print size in bytes \n",
    "print(f\"Size of dense array: {example.nbytes}\") \n",
    " \n",
    "# convert numpy array to sparse CSR matrix \n",
    "sparse_example = sparse.csr_matrix(example) \n",
    " \n",
    "# print size of this sparse matrix \n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\") \n",
    " \n",
    "full_size = ( \n",
    "    sparse_example.data.nbytes +  \n",
    "    sparse_example.indptr.nbytes +  \n",
    "    sparse_example.indices.nbytes \n",
    ") \n",
    " \n",
    "# print full size of this sparse matrix \n",
    "print(f\"Full size of sparse array: {full_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 8000000\n",
      "Full size of sparse array: 16000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing \n",
    " \n",
    "# create random 1-d array with 1001 different categories (int) \n",
    "example = np.random.randint(1000, size=1000000) \n",
    " \n",
    "# initialize OneHotEncoder from scikit-learn \n",
    "# keep sparse = False to get dense array \n",
    "ohe = preprocessing.OneHotEncoder(sparse=False) \n",
    " \n",
    "# fit and transform data with dense one hot encoder \n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1)) \n",
    " \n",
    "# print size in bytes for dense array \n",
    "print(f\"Size of dense array: {ohe_example.nbytes}\") \n",
    " \n",
    "# initialize OneHotEncoder from scikit-learn \n",
    "# keep sparse = True to get sparse array \n",
    "ohe = preprocessing.OneHotEncoder(sparse=True) \n",
    " \n",
    "# fit and transform data with sparse one-hot encoder \n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1)) \n",
    " \n",
    "# print size of this sparse matrix \n",
    "print(f\"Size of sparse array: {ohe_example.data.nbytes}\") \n",
    " \n",
    "full_size = ( \n",
    "    ohe_example.data.nbytes +  \n",
    "    ohe_example.indptr.nbytes + ohe_example.indices.nbytes \n",
    ") \n",
    " \n",
    "# print full size of this sparse matrix \n",
    "print(f\"Full size of sparse array: {full_size}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.ord_2 == \"Boiling Hot\"].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "0     84790\n",
       "1     97822\n",
       "2    142726\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "6    124239\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          67508\n",
       "1         124239\n",
       "2         142726\n",
       "3          64840\n",
       "4          97822\n",
       "           ...  \n",
       "599995    142726\n",
       "599996     84790\n",
       "599997    142726\n",
       "599998    124239\n",
       "599999     84790\n",
       "Name: id, Length: 600000, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].transform(\"count\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>0</td>\n",
       "      <td>15634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>1</td>\n",
       "      <td>17734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>2</td>\n",
       "      <td>26082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>3</td>\n",
       "      <td>12428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>4</td>\n",
       "      <td>11919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>5</td>\n",
       "      <td>3250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>6</td>\n",
       "      <td>22774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>19477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Expert</td>\n",
       "      <td>1</td>\n",
       "      <td>22956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Expert</td>\n",
       "      <td>2</td>\n",
       "      <td>33249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Expert</td>\n",
       "      <td>3</td>\n",
       "      <td>15792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Expert</td>\n",
       "      <td>4</td>\n",
       "      <td>15078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Expert</td>\n",
       "      <td>5</td>\n",
       "      <td>4225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Expert</td>\n",
       "      <td>6</td>\n",
       "      <td>28900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>0</td>\n",
       "      <td>13623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>1</td>\n",
       "      <td>15464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>2</td>\n",
       "      <td>22818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>3</td>\n",
       "      <td>10805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>4</td>\n",
       "      <td>10363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>5</td>\n",
       "      <td>2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>6</td>\n",
       "      <td>19899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>10800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Master</td>\n",
       "      <td>1</td>\n",
       "      <td>12364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>18035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>8594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Master</td>\n",
       "      <td>4</td>\n",
       "      <td>8209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Master</td>\n",
       "      <td>5</td>\n",
       "      <td>2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Master</td>\n",
       "      <td>6</td>\n",
       "      <td>15734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Novice</td>\n",
       "      <td>0</td>\n",
       "      <td>22718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Novice</td>\n",
       "      <td>1</td>\n",
       "      <td>26271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Novice</td>\n",
       "      <td>2</td>\n",
       "      <td>38233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Novice</td>\n",
       "      <td>3</td>\n",
       "      <td>17850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Novice</td>\n",
       "      <td>4</td>\n",
       "      <td>17373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Novice</td>\n",
       "      <td>5</td>\n",
       "      <td>4889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Novice</td>\n",
       "      <td>6</td>\n",
       "      <td>33263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ord_1  ord_2  count\n",
       "0   Contributor      0  15634\n",
       "1   Contributor      1  17734\n",
       "2   Contributor      2  26082\n",
       "3   Contributor      3  12428\n",
       "4   Contributor      4  11919\n",
       "5   Contributor      5   3250\n",
       "6   Contributor      6  22774\n",
       "7        Expert      0  19477\n",
       "8        Expert      1  22956\n",
       "9        Expert      2  33249\n",
       "10       Expert      3  15792\n",
       "11       Expert      4  15078\n",
       "12       Expert      5   4225\n",
       "13       Expert      6  28900\n",
       "14  Grandmaster      0  13623\n",
       "15  Grandmaster      1  15464\n",
       "16  Grandmaster      2  22818\n",
       "17  Grandmaster      3  10805\n",
       "18  Grandmaster      4  10363\n",
       "19  Grandmaster      5   2894\n",
       "20  Grandmaster      6  19899\n",
       "21       Master      0  10800\n",
       "22       Master      1  12364\n",
       "23       Master      2  18035\n",
       "24       Master      3   8594\n",
       "25       Master      4   8209\n",
       "26       Master      5   2262\n",
       "27       Master      6  15734\n",
       "28       Novice      0  22718\n",
       "29       Novice      1  26271\n",
       "30       Novice      2  38233\n",
       "31       Novice      3  17850\n",
       "32       Novice      4  17373\n",
       "33       Novice      5   4889\n",
       "34       Novice      6  33263"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_1\",\"ord_2\"])[\"id\"].count().reset_index(name=\"count\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Contributor_3\n",
       "1         Grandmaster_6\n",
       "2                 nan_2\n",
       "3              Novice_4\n",
       "4         Grandmaster_1\n",
       "              ...      \n",
       "599995         Novice_2\n",
       "599996         Novice_0\n",
       "599997    Contributor_2\n",
       "599998         Master_6\n",
       "599999    Contributor_0\n",
       "Name: new_feature, Length: 600000, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"new_feature\"] = (df.ord_1.astype(str) + \"_\" + df.ord_2.astype(str)) \n",
    "df.new_feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Contributor_3_c\n",
       "1         Grandmaster_6_e\n",
       "2                 nan_2_n\n",
       "3              Novice_4_a\n",
       "4         Grandmaster_1_h\n",
       "               ...       \n",
       "599995         Novice_2_a\n",
       "599996         Novice_0_n\n",
       "599997    Contributor_2_n\n",
       "599998         Master_6_m\n",
       "599999    Contributor_0_b\n",
       "Name: new_feature, Length: 600000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"new_feature\"] = (df.ord_1.astype(str) + \"_\"  + df.ord_2.astype(str)  + \"_\"  + df.ord_3.astype(str)) \n",
    "df.new_feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    142726\n",
       "6    124239\n",
       "1     97822\n",
       "0     84790\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    142726\n",
       "6    124239\n",
       "1     97822\n",
       "0     84790\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn import preprocessing \n",
    " \n",
    "# read training data \n",
    "train = pd.read_csv(\"train.csv\") \n",
    " \n",
    "#read test data \n",
    "test = pd.read_csv(\"test.csv\") \n",
    " \n",
    "# create a fake target column for test data \n",
    "# since this column doesn't exist \n",
    "test.loc[:, \"target\"] = -1 \n",
    " \n",
    "# concatenate both training and test data \n",
    "data = pd.concat([train, test]).reset_index(drop=True) \n",
    " \n",
    "# make a list of features we are interested in \n",
    "# id and target is something we should not encode \n",
    "features = [x for x in train.columns if x not in [\"id\", \"target\"]] \n",
    " \n",
    "# loop over the features list \n",
    "for feat in features: \n",
    "    # create a new instance of LabelEncoder for each feature \n",
    "    lbl_enc = preprocessing.LabelEncoder() \n",
    "     \n",
    "    # note the trick here \n",
    "    # since its categorical data, we fillna with a string \n",
    "    # and we convert all the data to string type \n",
    "    # so, no matter its int or float, its converted to string \n",
    "    # int/float but categorical!!! \n",
    "    temp_col = data[feat].fillna(\"NONE\").astype(str).values \n",
    " \n",
    "    # we can use fit_transform here as we do not \n",
    "    # have any extra test data that we need to \n",
    "    # transform on separately \n",
    "    data.loc[:, feat] = lbl_enc.fit_transform(temp_col) \n",
    "     \n",
    " \n",
    "# split the training and test data again     \n",
    "train = data[data.target != -1].reset_index(drop=True) \n",
    "test = data[data.target == -1].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    142726\n",
       "6    124239\n",
       "1     97822\n",
       "0     84790\n",
       "3     67508\n",
       "4     64840\n",
       "5     18075\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "G        3404\n",
       "V        3107\n",
       "J        1950\n",
       "L        1657\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ord_4 = df.ord_4.fillna(\"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000, \"ord_4\" ] = \"RARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "RARE     3607\n",
       "G        3404\n",
       "V        3107\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_folds.py \n",
    "# import pandas and model_selection module of scikit-learn \n",
    "import pandas as pd \n",
    "from sklearn import model_selection \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    " \n",
    "    # Read training data \n",
    "    df = pd.read_csv(\"train.csv\") \n",
    " \n",
    "    # we create a new column called kfold and fill it with -1 \n",
    "    df[\"kfold\"] = -1 \n",
    "     \n",
    "    # the next step is to randomize the rows of the data \n",
    "    df = df.sample(frac=1).reset_index(drop=True) \n",
    "     \n",
    "    # fetch labels \n",
    "    y = df.target.values \n",
    "     \n",
    "    # initiate the kfold class from model_selection module \n",
    "    kf = model_selection.StratifiedKFold(n_splits=5) \n",
    "     \n",
    "    # fill the new kfold column \n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)): \n",
    "        df.loc[v_, 'kfold'] = f \n",
    "     \n",
    "    # save the new csv with kfold column \n",
    "    df.to_csv(\"cat_train_folds.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    120000\n",
       "3    120000\n",
       "2    120000\n",
       "1    120000\n",
       "0    120000\n",
       "Name: kfold, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"cat_train_folds.csv\") \n",
    "df.kfold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==0].target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==1].target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==2].target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97536\n",
       "1    22464\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==3].target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97536\n",
       "1    22464\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==4].target.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7848471029437851\n"
     ]
    }
   ],
   "source": [
    "# ohe_logres.py \n",
    "import pandas as pd \n",
    " \n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"cat_train_folds.csv\") \n",
    " \n",
    "    # all columns are features except id, target and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnâ€™t matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # initialize OneHotEncoder from scikit-learn \n",
    "    ohe = preprocessing.OneHotEncoder() \n",
    " \n",
    "    # fit ohe on training + validation features \n",
    "    full_data = pd.concat( \n",
    "        [df_train[features], df_valid[features]], \n",
    "        axis=0 \n",
    "    ) \n",
    "    ohe.fit(full_data[features]) \n",
    " \n",
    "    # transform training data \n",
    "    x_train = ohe.transform(df_train[features]) \n",
    " \n",
    "    # transform validation data \n",
    "    x_valid = ohe.transform(df_valid[features]) \n",
    " \n",
    "    # initialize Logistic Regression model \n",
    "    model = linear_model.LogisticRegression()\n",
    "    \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.target.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(auc) \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    # run function for fold = 0 \n",
    "    # we can just replace this number and  \n",
    "    # run this for any fold \n",
    "    run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-57-22a3abeab124>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-22a3abeab124>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model = linear_model.LogisticRegression()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# ohe_logres.py \n",
    " \n",
    "    # initialize Logistic Regression model \n",
    "    model = linear_model.LogisticRegression() \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.target.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.7136460695836317\n",
      "Fold = 1, AUC = 0.7188040949443855\n",
      "Fold = 2, AUC = 0.715062332341312\n",
      "Fold = 3, AUC = 0.7161891418183647\n",
      "Fold = 4, AUC = 0.7186502470827778\n"
     ]
    }
   ],
   "source": [
    "# lbl_rf.py \n",
    "import pandas as pd \n",
    " \n",
    "from sklearn import ensemble \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    "def run(fold): \n",
    " \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"cat_train_folds.csv\") \n",
    " \n",
    "    # all columns are features except id, target and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # now its time to label encode the features \n",
    "    for col in features: \n",
    "         \n",
    "        # initialize LabelEncoder for each feature column\n",
    "        \n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "         \n",
    "        # fit label encoder on all data \n",
    "        lbl.fit(df[col]) \n",
    " \n",
    "        # transform all the data \n",
    "        df.loc[:, col] = lbl.transform(df[col]) \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # get training data \n",
    "    x_train = df_train[features].values \n",
    " \n",
    "    # get validation data \n",
    "    x_valid = df_valid[features].values \n",
    " \n",
    "    # initialize random forest model \n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1) \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.target.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/cat_train_folds.csv does not exist: '../input/cat_train_folds.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-21befe98e071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-21befe98e071>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# load the full training data with folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/cat_train_folds.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# all columns are features except id, target and kfold columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/cat_train_folds.csv does not exist: '../input/cat_train_folds.csv'"
     ]
    }
   ],
   "source": [
    "# ohe_svd_rf.py \n",
    "import pandas as pd \n",
    " \n",
    "from scipy import sparse \n",
    "from sklearn import decomposition \n",
    "from sklearn import ensemble \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\") \n",
    " \n",
    "    # all columns are features except id, target and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "        ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # initialize OneHotEncoder from scikit-learn \n",
    "    ohe = preprocessing.OneHotEncoder() \n",
    " \n",
    "    # fit ohe on training + validation features \n",
    "    full_data = pd.concat( \n",
    "        [df_train[features], df_valid[features]], \n",
    "        axis=0 \n",
    "    ) \n",
    "    ohe.fit(full_data[features]) \n",
    " \n",
    "    # transform training data \n",
    "    x_train = ohe.transform(df_train[features]) \n",
    " \n",
    "    # transform validation data \n",
    "    x_valid = ohe.transform(df_valid[features]) \n",
    " \n",
    "    # initialize Truncated SVD \n",
    "    # we are reducing the data to 120 components \n",
    "    svd = decomposition.TruncatedSVD(n_components=120) \n",
    " \n",
    "    # fit svd on full sparse training data \n",
    "    full_sparse = sparse.vstack((x_train, x_valid)) \n",
    "    svd.fit(full_sparse) \n",
    " \n",
    "    # transform sparse training data \n",
    "    x_train = svd.transform(x_train) \n",
    " \n",
    "    # transform sparse validation data \n",
    "    x_valid = svd.transform(x_valid) \n",
    " \n",
    "    # initialize random forest model \n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.target.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb.py \n",
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    " \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"cat_train_folds.csv\") \n",
    " \n",
    "    # all columns are features except id, target and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # now itâ€™s time to label encode the features \n",
    "    for col in features: \n",
    "         \n",
    "        # initialize LabelEncoder for each feature column \n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "         \n",
    "        # fit label encoder on all data \n",
    "        lbl.fit(df[col]) \n",
    " \n",
    "        # transform all the data \n",
    "        df.loc[:, col] = lbl.transform(df[col]) \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # get training data \n",
    "    x_train = df_train[features].values \n",
    " \n",
    "    # get validation data \n",
    "    x_valid = df_valid[features].values \n",
    " \n",
    "    # initialize xgboost model \n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,  \n",
    "        max_depth=7, \n",
    "        n_estimators=200 \n",
    "    ) \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.target.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50K    37155\n",
       ">50K     11687\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#https://www.kaggle.com/wenruliu/adult-income-dataset?select=adult.csv\n",
    "df = pd.read_csv(\"adult.csv\") \n",
    "df.income.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_logres.py \n",
    "import pandas as pd \n",
    " \n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    "\n",
    "def run(fold): \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"adult_folds.csv\") \n",
    " \n",
    "    # list of numerical columns \n",
    "    num_cols = [ \n",
    "        \"fnlwgt\", \n",
    "        \"age\", \n",
    "        \"capital.gain\", \n",
    "        \"capital.loss\", \n",
    "        \"hours.per.week\" \n",
    "    ] \n",
    " \n",
    "    # drop numerical columns \n",
    "    df = df.drop(num_cols, axis=1) \n",
    " \n",
    "    # map targets to 0s and 1s \n",
    "    target_mapping = { \n",
    "        \"<=50K\": 0, \n",
    "        \">50K\": 1 \n",
    "    } \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping) \n",
    " \n",
    "    # all columns are features except income and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # initialize OneHotEncoder from scikit-learn \n",
    "    ohe = preprocessing.OneHotEncoder() \n",
    " \n",
    "    # fit ohe on training + validation features \n",
    "    full_data = pd.concat( \n",
    "        [df_train[features], df_valid[features]], \n",
    "        axis=0) \n",
    "    ohe.fit(full_data[features]) \n",
    " \n",
    "    # transform training data \n",
    "    x_train = ohe.transform(df_train[features]) \n",
    " \n",
    "    # transform validation data \n",
    "    x_valid = ohe.transform(df_valid[features]) \n",
    " \n",
    "    # initialize Logistic Regression model \n",
    "    model = linear_model.LogisticRegression() \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.income.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb.py \n",
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    " \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\") \n",
    " \n",
    "    # list of numerical columns \n",
    "    num_cols = [ \n",
    "        \"fnlwgt\", \n",
    "        \"age\", \n",
    "        \"capital.gain\", \n",
    "        \"capital.loss\", \n",
    "        \"hours.per.week\" \n",
    "    ] \n",
    " \n",
    "    # drop numerical columns \n",
    "    df = df.drop(num_cols, axis=1) \n",
    " \n",
    "    # map targets to 0s and 1s \n",
    "    target_mapping = { \n",
    "        \"<=50K\": 0, \n",
    "        \">50K\": 1 \n",
    "    } \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping) \n",
    " \n",
    "    # all columns are features except kfold & income columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # now its time to label encode the features \n",
    "    for col in features: \n",
    "         \n",
    "        # initialize LabelEncoder for each feature column\n",
    "        lbl = preprocessing.LabelEncoder() \n",
    "         \n",
    "        # fit label encoder on all data \n",
    "        lbl.fit(df[col]) \n",
    " \n",
    "        # transform all the data \n",
    "        df.loc[:, col] = lbl.transform(df[col]) \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # get training data \n",
    "    x_train = df_train[features].values \n",
    " \n",
    "    # get validation data \n",
    "    x_valid = df_valid[features].values \n",
    " \n",
    "    # initialize xgboost model \n",
    "    model = xgb.XGBClassifier( \n",
    "        n_jobs=-1 \n",
    "    ) \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.income.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb_num.py \n",
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    " \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds \n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\") \n",
    " \n",
    "    # list of numerical columns\n",
    "    num_cols = [ \n",
    "        \"fnlwgt\", \n",
    "        \"age\", \n",
    "        \"capital.gain\", \n",
    "        \"capital.loss\", \n",
    "        \"hours.per.week\" \n",
    "    ] \n",
    " \n",
    "    # map targets to 0s and 1s \n",
    "    target_mapping = { \n",
    "        \"<=50K\": 0, \n",
    "        \">50K\": 1 \n",
    "    } \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping) \n",
    " \n",
    "    # all columns are features except kfold & income columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        # do not encode the numerical columns \n",
    "        if col not in num_cols: \n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # now its time to label encode the features \n",
    "    for col in features: \n",
    "        if col not in num_cols:         \n",
    "            # initialize LabelEncoder for each feature column \n",
    "            lbl = preprocessing.LabelEncoder() \n",
    "             \n",
    "            # fit label encoder on all data \n",
    "            lbl.fit(df[col]) \n",
    " \n",
    "            # transform all the data \n",
    "            df.loc[:, col] = lbl.transform(df[col]) \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # get training data\n",
    "    x_train = df_train[features].values \n",
    " \n",
    "    # get validation data \n",
    "    x_valid = df_valid[features].values \n",
    " \n",
    "    # initialize xgboost model \n",
    "    model = xgb.XGBClassifier( \n",
    "        n_jobs=-1 \n",
    "    ) \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.income.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb_num_feat.py \n",
    "import itertools \n",
    "import pandas as pd \n",
    "import xgboost as xgb \n",
    " \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    " \n",
    "def feature_engineering(df, cat_cols): \n",
    "    \"\"\" \n",
    "    This function is used for feature engineering \n",
    "    :param df: the pandas dataframe with train/test data \n",
    "    :param cat_cols: list of categorical columns \n",
    "    :return: dataframe with new features \n",
    "    \"\"\" \n",
    "    # this will create all 2-combinations of values \n",
    "    # in this list \n",
    "    # for example: \n",
    "    # list(itertools.combinations([1,2,3], 2)) will return \n",
    "    # [(1, 2), (1, 3), (2, 3)] \n",
    "    combi = list(itertools.combinations(cat_cols, 2)) \n",
    "    for c1, c2 in combi: \n",
    "        df.loc[ \n",
    "          :,  \n",
    "          c1 + \"_\" + c2 \n",
    "        ] = df[c1].astype(str) + \"_\" + df[c2].astype(str) \n",
    "    return df \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\") \n",
    " \n",
    "    # list of numerical columns \n",
    "    num_cols = [ \n",
    "        \"fnlwgt\", \n",
    "        \"age\", \n",
    "        \"capital.gain\", \n",
    "        \"capital.loss\", \n",
    "        \"hours.per.week\" \n",
    "    ] \n",
    " \n",
    "    # map targets to 0s and 1s \n",
    "    target_mapping = { \n",
    "        \"<=50K\": 0, \n",
    "        \">50K\": 1 \n",
    "    } \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping) \n",
    " \n",
    "    # list of categorical columns for feature engineering \n",
    "    cat_cols = [ \n",
    "        c for c in df.columns if c not in num_cols \n",
    "        and c not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # add new features \n",
    "    df = feature_engineering(df, cat_cols) \n",
    " \n",
    "    # all columns are features except kfold & income columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        # do not encode the numerical columns \n",
    "        if col not in num_cols: \n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # now its time to label encode the features \n",
    "    for col in features: \n",
    "        if col not in num_cols:         \n",
    "            # initialize LabelEncoder for each feature column \n",
    "            lbl = preprocessing.LabelEncoder() \n",
    "             \n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col]) \n",
    " \n",
    "            # transform all the data \n",
    "            df.loc[:, col] = lbl.transform(df[col]) \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # get training data \n",
    "    x_train = df_train[features].values \n",
    " \n",
    "    # get validation data \n",
    "    x_valid = df_valid[features].values \n",
    " \n",
    "    # initialize xgboost model \n",
    "    model = xgb.XGBClassifier( \n",
    "        n_jobs=-1 \n",
    "    ) \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.income.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    for fold_ in range(5): \n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_encoding.py \n",
    "import copy \n",
    "import pandas as pd \n",
    " \n",
    "from sklearn import metrics \n",
    "from sklearn import preprocessing \n",
    "import xgboost as xgb \n",
    " \n",
    "def mean_target_encoding(data): \n",
    " \n",
    "    # make a copy of dataframe \n",
    "    df = copy.deepcopy(data) \n",
    " \n",
    "    # list of numerical columns \n",
    "    num_cols = [ \n",
    "        \"fnlwgt\", \n",
    "        \"age\", \n",
    "        \"capital.gain\", \n",
    "        \"capital.loss\", \n",
    "        \"hours.per.week\" \n",
    "    ] \n",
    " \n",
    "    # map targets to 0s and 1s \n",
    "    target_mapping = { \n",
    "        \"<=50K\": 0, \n",
    "        \">50K\": 1 \n",
    "    } \n",
    " \n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping) \n",
    "     \n",
    "    # all columns are features except income and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "        and f not in num_cols \n",
    "    ] \n",
    " \n",
    "    # all columns are features except kfold & income columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        # do not encode the numerical columns \n",
    "        if col not in num_cols: \n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    "     \n",
    "    # now its time to label encode the features \n",
    "    for col in features: \n",
    "        if col not in num_cols:         \n",
    "            # initialize LabelEncoder for each feature column \n",
    "            lbl = preprocessing.LabelEncoder() \n",
    "             \n",
    "            # fit label encoder on all data \n",
    "            lbl.fit(df[col]) \n",
    " \n",
    "            # transform all the data \n",
    "            df.loc[:, col] = lbl.transform(df[col]) \n",
    " \n",
    "    # a list to store 5 validation dataframes \n",
    "    encoded_dfs = [] \n",
    " \n",
    "    # go over all folds \n",
    "    for fold in range(5): \n",
    "        # fetch training and validation data \n",
    "        df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    "        df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    "        # for all feature columns, i.e. categorical columns \n",
    "        for column in features: \n",
    "            # create dict of category:mean target \n",
    "            mapping_dict = dict( \n",
    "                df_train.groupby(column)[\"income\"].mean() \n",
    "            ) \n",
    "            # column_enc is the new column we have with mean encoding \n",
    "            df_valid.loc[ \n",
    "                :, column + \"_enc\" \n",
    "            ] = df_valid[column].map(mapping_dict) \n",
    "        # append to our list of encoded validation dataframes \n",
    "        encoded_dfs.append(df_valid) \n",
    "    # create full data frame again and return \n",
    "    encoded_df = pd.concat(encoded_dfs, axis=0) \n",
    "    return encoded_df \n",
    " \n",
    "def run(df, fold): \n",
    "    # note that folds are same as before \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # all columns are features except income and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") \n",
    "    ] \n",
    " \n",
    "    # scale training data \n",
    "    x_train = df_train[features].values \n",
    " \n",
    "    # scale validation data \n",
    "    x_valid = df_valid[features].values \n",
    " \n",
    "    # initialize xgboost model \n",
    "    model = xgb.XGBClassifier( \n",
    "        n_jobs=-1, \n",
    "        max_depth=7 \n",
    "    ) \n",
    " \n",
    "    # fit model on training data (ohe) \n",
    "    model.fit(x_train, df_train.income.values) \n",
    " \n",
    "    # predict on validation data \n",
    "    # we need the probability values as we are calculating AUC \n",
    "    # we will use the probability of 1s \n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1] \n",
    " \n",
    "    # get roc auc score \n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds) \n",
    " \n",
    "    # print auc \n",
    "    print(f\"Fold = {fold}, AUC = {auc}\") \n",
    " \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    # read data \n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\") \n",
    "     \n",
    "    # create mean target encoded categories and \n",
    "    # munge data \n",
    "    df = mean_target_encoding(df) \n",
    " \n",
    "    # run training and validation for 5 folds\n",
    "    for fold_ in range(5): \n",
    "        run(df, fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_emebddings.py \n",
    "import os \n",
    "import gc \n",
    "import joblib \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn import metrics, preprocessing \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import optimizers \n",
    "from tensorflow.keras.models import Model, load_model \n",
    "from tensorflow.keras import callbacks \n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras import utils \n",
    " \n",
    "def create_model(data, catcols): \n",
    "    \"\"\" \n",
    "    This function returns a compiled tf.keras model \n",
    "    for entity embeddings \n",
    "    :param data: this is a pandas dataframe \n",
    "    :param catcols: list of categorical column names \n",
    "    :return: compiled tf.keras model \n",
    "    \"\"\" \n",
    "    # init list of inputs for embeddings \n",
    "    inputs = [] \n",
    " \n",
    "    # init list of outputs for embeddings \n",
    "    outputs = [] \n",
    " \n",
    "    # loop over all categorical columns \n",
    "    for c in catcols: \n",
    "        # find the number of unique values in the column \n",
    "        num_unique_values = int(data[c].nunique()) \n",
    "        # simple dimension of embedding calculator \n",
    "        # min size is half of the number of unique values \n",
    "        # max size is 50. max size depends on the number of unique \n",
    "        # categories too. 50 is quite sufficient most of the times \n",
    "        # but if you have millions of unique values, you might need \n",
    "        # a larger dimension \n",
    "        embed_dim = int(min(np.ceil((num_unique_values)/2), 50)) \n",
    " \n",
    "        # simple keras input layer with size 1 \n",
    "        inp = layers.Input(shape=(1,)) \n",
    " \n",
    "        # add embedding layer to raw input \n",
    "        # embedding size is always 1 more than unique values in input \n",
    "        out = layers.Embedding( \n",
    "            num_unique_values + 1, embed_dim, name=c \n",
    "        )(inp) \n",
    "         \n",
    "        # 1-d spatial dropout is the standard for emebedding layers  \n",
    "        # you can use it in NLP tasks too \n",
    "        out = layers.SpatialDropout1D(0.3)(out) \n",
    " \n",
    "        # reshape the input to the dimension of embedding\n",
    "     # this becomes our output layer for current feature \n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out) \n",
    " \n",
    "        # add input to input list \n",
    "        inputs.append(inp) \n",
    " \n",
    "        # add output to output list \n",
    "        outputs.append(out) \n",
    "     \n",
    "    # concatenate all output layers \n",
    "    x = layers.Concatenate()(outputs) \n",
    " \n",
    "    # add a batchnorm layer. \n",
    "    # from here, everything is up to you \n",
    "    # you can try different architectures \n",
    "    # this is the architecture I like to use \n",
    "    # if you have numerical features, you should add \n",
    "    # them here or in concatenate layer \n",
    "    x = layers.BatchNormalization()(x) \n",
    "     \n",
    "    # a bunch of dense layers with dropout. \n",
    "    # start with 1 or two layers only \n",
    "    x = layers.Dense(300, activation=\"relu\")(x) \n",
    "    x = layers.Dropout(0.3)(x) \n",
    "    x = layers.BatchNormalization()(x) \n",
    "     \n",
    "    x = layers.Dense(300, activation=\"relu\")(x) \n",
    "    x = layers.Dropout(0.3)(x) \n",
    "    x = layers.BatchNormalization()(x) \n",
    "     \n",
    "    # using softmax and treating it as a two class problem \n",
    "    # you can also use sigmoid, then you need to use only one \n",
    "    # output class \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x) \n",
    " \n",
    "    # create final model \n",
    "    model = Model(inputs=inputs, outputs=y) \n",
    " \n",
    "    # compile the model \n",
    "    # we use adam and binary cross entropy. \n",
    "    # feel free to use something else and see how model behaves \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam') \n",
    "    return model \n",
    " \n",
    "def run(fold): \n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\") \n",
    " \n",
    "    # all columns are features except id, target and kfold columns \n",
    "    features = [ \n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\") \n",
    "    ] \n",
    " \n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\" \n",
    "    # it doesnt matter because all are categories \n",
    "    for col in features: \n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") \n",
    " \n",
    "    # encode all features with label encoder individually \n",
    "    # in a live setting you need to save all label encoders \n",
    "    for feat in features: \n",
    "        lbl_enc = preprocessing.LabelEncoder() \n",
    "        df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values) \n",
    " \n",
    "    # get training data using folds \n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True) \n",
    " \n",
    "    # get validation data using folds \n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True) \n",
    " \n",
    "    # create tf.keras model \n",
    "    model = create_model(df, features) \n",
    " \n",
    "    # our features are lists of lists \n",
    "    xtrain = [ \n",
    "        df_train[features].values[:, k] for k in range(len(features)) \n",
    "    ] \n",
    "    xvalid = [ \n",
    "        df_valid[features].values[:, k] for k in range(len(features)) \n",
    "    ] \n",
    "    # fetch target columns \n",
    "    ytrain = df_train.target.values \n",
    "    yvalid = df_valid.target.values \n",
    " \n",
    "    # convert target columns to categories \n",
    "    # this is just binarization \n",
    "    ytrain_cat = utils.to_categorical(ytrain) \n",
    "    yvalid_cat = utils.to_categorical(yvalid) \n",
    "     \n",
    "    # fit the model \n",
    "    model.fit(xtrain, \n",
    "              ytrain_cat,\n",
    "              validation_data=(xvalid, yvalid_cat), \n",
    "              verbose=1, \n",
    "              batch_size=1024, \n",
    "              epochs=3 \n",
    "             ) \n",
    " \n",
    "    # generate validation predictions \n",
    "    valid_preds = model.predict(xvalid)[:, 1] \n",
    " \n",
    "    # print roc auc score \n",
    "    print(metrics.roc_auc_score(yvalid, valid_preds)) \n",
    " \n",
    "    # clear session to free up some GPU memory \n",
    "    K.clear_session() \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    run(0) \n",
    "    run(1) \n",
    "    run(2) \n",
    "    run(3) \n",
    "    run(4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
